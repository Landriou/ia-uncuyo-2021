# La inteligencia artificial débil 
La IA se fundó en el supuesto de que es posible una IA débil y algunas personas afirmaron que la IA débil pero esto puede variar en base a cómo la IA es definida.
Una posible definición de IA “la búsqueda del mejor programa de un agente en una arquitectura determinada.” Bajo esta definición la IA es posible
por ejemplo para cualquier arquitectura digital con k bits de programas hay exactamente 2k de agentes , y todo lo que tenemos que hacer para encontrar el mejor es enumerar y probar. Esto no es posible en la práctica. Sin embargo los filósofos están interesados en la parte teórica de esto, más en la comparación de las 2 arquitecturas entre humano y máquina y sobre la pregunta “las máquinas pueden pensar?”
Eso lleva a varias respuestas, una muy conocida fue la de Dijkstra “ Que las máquinas puedan pensar es tan relevante como si los submarinos puedan nadar” o otras cuestiones filosóficas como el significado de la palabra “pensar”, pensar realmente necesita un cerebro o partes de él?
Alan turing se pronunció sobre este tema con el test de turing, que consiste en que las máquinas pasarán un test de inteligencia llamado el test de turing, que consiste en que la máquina engañara a una persona en una conversación vía mensajes por 5 minutos sin que la persona pudiera darse cuenta que era una máquina con quien estaba chateando, sin embargo esto no era tan preciso, la persona interrogada debería ser un juez bien sofisticado para tomarlo en cuenta.

El argumento de la discapacidad, hace referencia a que una máquina nunca podrá hacer algo X, por ejemplo ser amable,hermosa, amigable, tener iniciativa, etc.
Sin embargo, hay computadoras, que han hecho muchísimos descubrimientos en amplios campos e incluso que juegan ajedrez de forma increíble, pero esto no implica que las computadoras tengan entendimiento o conocimiento de lo que están haciendo.

El argumento de la informalidad fue planteado por alan turing, como un reclamo en el cual el comportamiento humano es demasiado complejo para ser caputadora por un conjunto de simples reglas y por esa razón computadoras no pueden hacer más que seguir este set de reglas, las máquinas no pueden generar comportamiento tan inteligente como los de un humano. 
La incapacidad de capturar todo en un set de instrucciones lógicas es llamada el problema de calificación en IA.

Dreyfus & Dreyfus habla sobre un argumento basado sobre agentes situados versus los agentes incorporeos en los motores de inferencia lógica, por ejemplo existen declaraciones de inferencia lógica como un perro implica que es un mamífero, esto es una desventaja frente a un agente que ha visto el perro correr, ha jugado con ellos y observado continuamente. Para entender cómo funcionan los agentes humanos, tenemos que considerar el agente entero, no solo el programa agente.
Para esto, tomamos un enfoque de cognición incorporada, el cual trata de que no tiene sentido tomar el cerebro por separado, ya que la cognición toma lugar dentro de un cuerpo, que está embebido en un entorno, es necesario estudiar el sistema como algo completo, el cerebro aumenta su razonamiento refiriéndose al medio. Bajo el programa de cognición incorporada, la robótica, la visión y otros sensores se vuelven centrales, no periféricos.

# La inteligencia artificial fuerte

Muchos filósofos reclaman que una máquina que pasa el test de Turing no está realmente pensando, solo está simulando el pensamiento.
Turing llama esto, el argumento de la conciencia , la máquina tiene que estar consciente de sus propios estados mentales y acciones pero mientras la conciencia es un sujeto importante, autores dicen que la máquina debería realmente sentir emociones. Otros autores se enfocan en la intencionalidad de la máquina sobre algo en el mundo real.
La respuesta de Turing sobre esto, fue sobre que tenemos la costumbre de pensar que todas las persona realmente piensan, en contraposición de plantear si las máquinas pueden pensar realmente.
Turing sostuvo que el problema realmente va a desaparecer con el tiempo ya que las máquinas van a llegar a un nivel de sostificacion en el cual no se va a distinguir entre IA fuerte e IA débil, contra esto, se puede decir, los humanos tienen mentes reales y las máquinas pueden o no tenerlas, debido al nivel de sofisticacion de las máquinas en un futuro. Para resolver esto, es necesario entender cómo los humanos tienen mentes reales, no solo cuerpos, han habido muchos esfuerzos físicos para resolver este problema mente-cuerpo.
Rene descartes, concluida una teoría dualista en la cual la mente y el cuerpo existen en 2 reinos separados. la pregunta surte como la mente controla el cuerpo si realmente existen de forma separadas.
La teoría monista, dice que la mente no está separada del cuerpo y los estados mentales son estados físicos, por ejemplo “comer una hamburguesa”.

Muchos físicos filósofos han tratado de explicar que es un estado mental, y se enfocan en los estados intencionales, estos son como “creen, saber, desear,etc”.
Por ejemplo, comer una hamburguesa y realmente creer que es una hamburguesa y que está pasando con ella.

Funcionalismo y el reemplazamiento de un cerebro

La teoría del funcionamiento dice que un estado mental es cualquier condición intermedia causada entre una entrada y una salida, bajo esta teoría funcionalista, cualquier 2 sistemas con procesos isomórficos tendrían los mismos estados mentales, entonces, un programa de computadora podría tener los mismos estados mentales que una persona. En un nivel de abstracción en el cual la implementación no se tiene en cuenta.
Los reclamos del funcionalismo, fueron representados en un experimento en el cual se reemplazó un cerebro, en el cual se tomaba como hipótesis una situación en el cual la neurociencia se ha desarrollado a tal punto de que todas los comportamientos de entrada y salida de todas nuestras neuronas del ser humano, son perfectamente entendidas. Además de esto, se supone que se pueden construir microscopios dispositivos electrónicos que pueden simular todo el comportamiento neuronal y pueden ser reemplazadas por todas las neuronas de un cerebro sin interrumpir el cerebro entero.
Por la definición del experimento, el estado externo del sujeto debería permanecer sin cambios algunos comparados con lo que él observaría si aun conserva su antiguas neuronas, aunque la presencia o ausencia de conciencia no puede ser determinada fácilmente por un tercero, el sujeto del experimento debería al menos ser capaz de registrar cualquier cambio en su propia experiencia consciente. Aparentemente, hay un choque directo de intuiciones sobre lo que sucedería. Moravec, investigador de robótica y funcionalista, está convencido de que su conciencia no se verá afectada. Searle, un filósofo
y naturalista biológico, está igualmente convencido de que su conciencia se desvanecerá:
este experimento tiene tres posibles conclusiones:
1. Los mecanismos causales de la conciencia que generan este tipo de salidas en los cerebros normales siguen funcionando en la versión electrónica, que por tanto es consciente.
2. Los eventos mentales conscientes en el cerebro normal no tienen una conexión causal con la conducta, y estos faltan en el cerebro electrónico, que por lo tanto no es consciente.
3. El experimento es imposible y, por lo tanto, la especulación al respecto no tiene sentido.
Aunque no se puede descartar la segunda posibilidad, reduce la conciencia a lo que los filósofos llaman un papel epifenoménico, algo que sucede, pero que no proyecta sombra, por así decirlo, en el mundo observable.


Un gran challenge hacia el funcionalismo es el naturalismo biológico según el cual los estados mentales son características emergentes de alto nivel causadas por procesos físicos de bajo nivel en las neuronas, y son las propiedades (no especificadas) de las neuronas las que importan. Por tanto, los estados mentales no se pueden duplicar simplemente sobre la base de que algún programa tenga la misma estructura funcional con el mismo comportamiento de entrada-salida; requerimos que el programa se ejecute en una arquitectura con el mismo poder causal que las neuronas.

. El verdadero reclamo hecho por un autor basado en el naturalismo biológico se basa en 4 axiomas:
1. Los programas de computadora son formales (sintácticos).
2. Las mentes humanas tienen contenidos mentales (semántica).
3. La sintaxis por sí misma no es constitutiva ni suficiente para la semántica.
4. Los cerebros causan mentes.
A partir de los tres primeros axiomas, el autor concluye que los programas no son suficientes para las mentes. En otras palabras, un agente que ejecuta un programa puede ser una mente, pero no es necesariamente una mente por el mero hecho de ejecutar el programa. Del cuarto axioma concluye: "Cualquier otro sistema capaz de causar mentes tendría que tener poderes causales (al menos) equivalentes a los de los cerebros". A partir de ahí, infiere que cualquier cerebro artificial tendría que duplicar los poderes causales de los cerebros, no solo ejecutar un programa en particular, y que los cerebros humanos no producen fenómenos mentales únicamente en virtud de ejecutar un programa.
Los axiomas son controvertidos. Por ejemplo, los axiomas 1 y 2 se basan en una distinción no especificada entre sintaxis y semántica que parece estar estrechamente relacionada con la distinción entre contenido restringido y amplio. Por un lado, podemos ver las computadoras manipulando símbolos sintácticos; por el otro, podemos verlos como manipuladores de corriente eléctrica, que es lo que hacen principalmente los cerebros (según nuestro conocimiento actual). Así que parece que también podríamos decir que los cerebros son sintácticos.
Suponiendo que seamos generosos al interpretar los axiomas, entonces se sigue la conclusión de que los programas no son suficientes para las mentes. Pero la conclusión es insatisfactoria: todo lo que el autor ha demostrado es que si niega explícitamente el funcionalismo (eso es lo que hace su axioma 3), entonces no se puede concluir necesariamente que los no cerebros sean mentes. Esto es bastante razonable, casi tautológico, por lo que todo el argumento se reduce a si el axioma 3 puede aceptarse.


# La etica y riesgos en el desarrollo de la IA

Hasta ahora, nos hemos concentrado en si podemos desarrollar IA, pero también debemos considerar si deberíamos hacerlo. Si es más probable que los efectos de la tecnología de la inteligencia artificial sean negativos que positivos, entonces sería responsabilidad moral de los trabajadores en el campo reorientar su investigación.
Muchas nuevas tecnologías han tenido efectos secundarios negativos no deseados: la fisión nuclear trajo Chernobyl y la amenaza de destrucción global; el motor de combustión interna trajo la contaminación del aire, el calentamiento global y la pavimentación del paraíso. En cierto sentido, los automóviles son
robots que han conquistado el mundo haciéndose indispensables. Todos los científicos e ingenieros enfrentan consideraciones éticas sobre cómo deben actuar en el trabajo, qué proyectos deben o no deben realizarse y cómo deben manejarse. La IA, sin embargo, parece plantear algunos problemas nuevos más allá de, digamos, construir puentes que no se caigan:
• Las personas pueden perder sus trabajos debido a la automatización.
• La gente puede tener demasiado (o muy poco) tiempo libre.
• Las personas pueden perder su sentido de ser únicas.
• Los sistemas de inteligencia artificial pueden usarse para fines no deseados.
• El uso de sistemas de IA puede resultar en una pérdida de responsabilidad.
• El éxito de la IA podría significar el fin de la raza humana

Podríamos centrarnos en el último que es el más preocupante
La pregunta es si un sistema de inteligencia artificial presenta un riesgo mayor que el software tradicional. Examinaremos tres fuentes de riesgo.
Primero, la estimación del estado del sistema de inteligencia artificial puede ser incorrecta, lo que hace que haga algo incorrecto. Por ejemplo, un automóvil autónomo podría estimar incorrectamente la posición de un automóvil en el carril adyacente, lo que provocaría un accidente que podría matar a los ocupantes. Más en serio, un misil
El sistema de defensa podría detectar erróneamente un ataque y lanzar un contraataque, provocando la muerte de miles de millones. Estos riesgos no son realmente riesgos de los sistemas de inteligencia artificial; en ambos casos, un humano o una computadora podrían cometer el mismo error con la misma facilidad. La forma correcta de mitigar estos riesgos es diseñar un sistema con controles y contrapesos para que un solo error de estimación de estado no se propague a través del sistema sin verificar.
En segundo lugar, especificar la función de utilidad correcta para que un sistema de IA la maximice no es tan fácil. Por ejemplo, podríamos proponer una función de utilidad diseñada para minimizar el sufrimiento humano, expresada como una función de recompensa aditiva a lo largo del tiempo como en el Capítulo 17. Sin embargo, dada la forma en que somos los humanos, siempre encontraremos la manera de sufrir incluso en el paraíso; de modo que la decisión óptima para el sistema de inteligencia artificial es acabar con la raza humana lo antes posible: no hay humanos, no hay sufrimiento.
Con los sistemas de IA, entonces, debemos tener mucho cuidado con lo que pedimos, mientras que los humanos no tendrían problemas para darse cuenta de que la función de utilidad propuesta no puede tomarse literalmente. Por otro lado, las computadoras no necesitan estar contaminadas por comportamientos irracionales.
Los humanos a veces usan su inteligencia de manera agresiva porque los humanos tienen algunas tendencias innatamente agresivas, debido a la selección natural. Las máquinas que construimos no necesitan ser innatamente agresivas, a menos que decidamos construirlas de esa manera (oa menos que surjan como el producto final de un diseño de mecanismo que fomente el comportamiento agresivo). Afortunadamente, existen técnicas, como el aprendizaje de aprendices, que nos permiten especificar una función de utilidad por
ejemplo. Uno puede esperar que un robot que sea lo suficientemente inteligente como para descubrir cómo acabar con la raza humana también sea lo suficientemente inteligente como para darse cuenta de que esa no era la función de utilidad prevista.
En tercer lugar, la función de aprendizaje del sistema de inteligencia artificial puede hacer que se convierta en un sistema con un comportamiento no deseado. Este escenario es el más serio y exclusivo de los sistemas de IA.

# Map mind
https://www.mindomo.com/mindmap/3cca6a01808444f09a02a7b2643cda1a

# Opinión Final:
Realmente me ha hecho pensar un monton el haber leido esto, de hecho, incluso empecé a pensar si realmente los animales fuera de los humanos, como gatos o perros, tienen estados mentales o no, segun investigue, al parecer si tienen cierto tipo de inteligencia aunque está basada en instintos y reconocimiento de patrones.
El trasfondo filosófico de las IAs me parece increíble, me sorprende por alguna razón ver que el nombre de Alan Turing se repite tanto en el tema siendo que vivió tan poco, me lleva a pensar qué hubiera pasado si hubiera vivido su vida entera.
Concuerdo con que en un futuro la pregunta de si las máquinas piensan va a hacer irrelevante con el ritmo actual que lleva la tecnología, quizás ellos no lo alcanzaron a ver, pero es impresionante que sus predicciones y pensamientos se extiendan hasta nuestra época y más allá, incluso llegó a pensar que quizás las IAs terminan teniendo derechos o algunas cosas de ciencia ficción.
Me parece bastante acertado pensar en que el cerebro es una máquina sintáctica que en definitiva funciona con pulsos eléctricos casi como las computadoras, pero como dicen algunos autores, creo que hay que tomarlo como un sistema completo y no solo el cerebro.
Respecto a la ética y el riesgo de las IAs, al estar metido un poco en toda la industria me he dado cuenta que todo va a avanzar en cuestiones de que es lo que mas rente para la misma industria, por eso creo que las profesiones realmente van a terminar desapareciendo, lo cual no me parece mal, el hecho utópico de que todos se puedan dedicar al arte o a lo que realmente les gusta me parece algo fantástico, pero por otro lado pienso en la parte que menciona el libro de que la fisión nuclear y los automóviles tuvieron su costo para la humanidad y que aún no conocemos realmente cuál va a ser el costo de la IA.
En fin, me pone contento poder vivir todo esto metido en el medio de la vorágine de la tecnología, pienso que hay mucha gente que no se da cuenta de todo lo que está pasando con IA, Blockchain, NFTs y demás cosas
